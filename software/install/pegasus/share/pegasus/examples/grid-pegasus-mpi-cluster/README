This example illustrates how to run pegasus-mpi-cluster (PMC) on a remote 
grid site using Globus GRAM and GridFTP. In this example we are using NICS
Kraken as the target site. Kraken is a Cray XT5. PMC should work on all 
recent clusters with the exception of IBM BlueGene supercomputers, which
do not support fork() on worker nodes.

You will need to compile and install pegasus-mpi-cluster on the target
system. It is in the Pegasus source distribution under 
src/tools/pegasus-mpi-cluster. You may need to make changes to the 
Makefile to specify the correct MPI compiler wrapper for your target
system. On Kraken, for example, the MPI compiler is CC.

Once PMC is compiled and installed you will need to create a wrapper 
script that invokes the appropriate mpiexec for your target system. In
the case of Kraken the mpiexec is aprun. There is an example wrapper
script in this directory called pegasus-mpi-cluster-wrapper. We recommend
using the wrapper with the GRAM jobtype=single instead of using the 
PMC binary with jobtype=mpi because it allows you to have more control
over the placement of your processes and gives you access to PMC's 
command-line arguments.

Add an entry to your transformation catalog specifying the location of the 
PMC wrapper script on the target system. The transformation entry will 
need to specify the max wall time of the PMC job, the number of nodes, 
and the number of processes.

You need to create a grid proxy in order to use GridFTP and submit jobs
via GRAM.

The files are:
    plan        This script generates the DAX and plans and submits the
                workflow
    daxgen.py   This script actually generates the DAX
    sites.xml   This is the site catalog
    tc.data     This is the transformation catalog
    pegasusrc   This is the pegasus config file (properties file)
    dax.xml     This is the DAX generated by daxgen.py
    pegasus-mpi-cluster-wrapper This is an example wrapper for PMC


Purpose
	- An example to highlight a MPI submission directly to PBS via GLITE 
	interface in HTCondor and Pegasus. This example takes a Montage workflow and 
	clusters it into a single Pegasus MPI Cluster MPI job in the executable 
	workflow generated by Pegasus. 
	
	- In the generated tc the pegasus::mpiexec entry illustrates the various 
	profiles you can set to specify the behaviour of your MPI job.

	- In the glite configuration, for each MPI job you need to have a 
	simple wrapper that does mpiexec on your executable. For this example that 
	wrapper file is called pegasus-mpi-cluster-wrapper. 

	
Tested with
       - Pegasus 4.4.0
       - Condor 7.9.3 . Should work with stable condor release also.


Before you RUN
        - Put the pbs_local_submit_attributes.sh file distributed with
       Pegasus in share/pegasus/htcondor/glite directory in the GLITE bin
       directory of the  HTCondor installation.

       - GLITE directory can be determined by running
       condor_config_val GLITE_LOCATION
       
       - Update the values in the SETTINGS section of submit script.


HOW TO RUN on USC HPCC
    # source the pegasus installation on USC HPCC
     source /home/rcf-proj/gmj/pegasus/SOFTWARE/setup.sh	
     
    # the example directory should be in your home directory

    ./submit will generate the workflow , plan it with pegasus and
    submit it for execution

    the sites.xml file will have a HPCC site that designates the HPCC cluster.
    
    #the outputs of the worklfow will appear in the outputs directory       
